# 3.2. 3D Reconstruction Pipeline

This section details the algorithms and implementation of the 3D reconstruction process in Brush.

## 3.2.1. Conceptual Overview

Brush performs 3D reconstruction using the **3D Gaussian Splatting** technique. Unlike traditional mesh-based or volumetric methods (like NeRF), Gaussian Splatting represents the scene as a collection of 3D Gaussians.

The core idea is to optimize the parameters (position, shape, color, opacity) of these Gaussians directly, typically starting from sparse points generated by Structure from Motion (SfM).

The process generally involves:

1.  **Input:** Posed images (camera parameters known) from a format like COLMAP or Nerfstudio.
2.  **Initialization:** Often starts with a sparse point cloud derived from SfM (possibly generated externally or by COLMAP itself).
3.  **Optimization:** Iteratively render the Gaussians from the training camera viewpoints and compare the result to the ground truth images. The differences (loss) are used to update the Gaussian parameters (position, covariance, color, opacity) via gradient descent. This involves a differentiable renderer.
4.  **Adaptive Density Control:** During optimization, Gaussians are periodically densified (split or cloned) in areas with large reconstruction errors and pruned (removed) if they are transparent or excessively large.
5.  **Output:** A set of optimized 3D Gaussians representing the scene, typically saved as a `.ply` file.

## 3.2.2. Algorithm(s)

The core reconstruction algorithm implemented in Brush is **3D Gaussian Splatting optimization**. The process starts with an initial set of Gaussians (either randomly generated within the scene bounds or loaded from an initial point cloud like `input.ply` often found in COLMAP datasets) and directly optimizes their parameters (position, rotation, scale, color, opacity, spherical harmonics) to minimize the difference between rendered views and the input training images.

Based on the current codebase analysis (primarily `brush-process`, `brush-train`), there is no evidence of alternative reconstruction pipelines like traditional Structure-from-Motion (SfM) + Multi-View Stereo (MVS) or Neural Radiance Fields (NeRF) being implemented. The focus is solely on the optimization of the Gaussian representation.

**References:**

*   **Original Paper:** [3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)

## 3.2.3. Implementation Details

The core reconstruction logic resides primarily within the `brush-train` crate.

*   **Burn Usage:** The [Burn](core-technologies.md#343-burn) framework is used for:
    *   Defining the optimizable parameters (Gaussians).
    *   Calculating losses between rendered images and training images.
    *   Performing automatic differentiation to get gradients.
    *   Applying optimizers (e.g., Adam) to update Gaussian parameters.
    *   Leveraging the `wgpu` backend for GPU acceleration.
*   **Crate Roles:**
    *   `brush-dataset`: Provides the input posed images and camera parameters.
    *   `brush-process`: May handle initial loading and any necessary preprocessing.
    *   `brush-train`: Orchestrates the optimization loop.
    *   `brush-render` / `brush-render-bwd`: Provide the differentiable rendering mechanism.
    *   `brush-kernel` / `brush-sort` / etc.: Provide GPU-accelerated primitives used by the renderer and potentially the optimizer.
*   **Data Structures:**
    *   **Core Crates:** The main logic resides in:
        *   `brush-process`: Orchestrates the loading and training stream (`train_stream.rs`).
        *   `brush-dataset`: Handles loading datasets (`load_dataset`), providing training batches (`SceneLoader`), and defining scene structures.
        *   `brush-train`: Contains the `SplatTrainer` responsible for the optimization loop (`step`, `refine_if_needed`) and evaluation (`eval_stats`).
        *   `brush-render`: Defines the `Splats` data structure and provides the forward rendering pass used in training loss calculation.
        *   `brush-render-bwd`: Provides the backward rendering pass for gradient computation.
    *   **Burn Framework:** [Burn](core-technologies.md#343-burn) is used for:
        *   Defining and managing tensors holding Gaussian parameters on the target backend (WGPU).
        *   Automatic differentiation to compute gradients during the backward pass.
        *   Providing optimizers (e.g., Adam) to update Gaussian parameters.
    *   **Key Data Structures:**
        *   `brush_render::gaussian_splats::Splats<B: Backend>`: Stores the parameters for all Gaussians (position, scale, rotation, opacity, color/SH) as Burn Tensors.
        *   `brush_dataset::Dataset`: Top-level structure holding training and evaluation `Scene` data.
        *   `brush_dataset::scene::Scene`: Contains a collection of `SceneView`s and scene metadata (bounds, up-vector).
        *   `brush_dataset::scene::SceneView`: Represents a single camera view with its `Camera` parameters and associated ground truth image data.
        *   `brush_render::camera::Camera`: Holds camera intrinsics and extrinsics.
        *   `brush_train::train::TrainBatch`: (Likely structure) Holds the data (views, images) for a single training step/batch.

## 3.2.4. Configuration

The training process can be configured through various command-line options when running `brush_app`. Key parameters influencing the reconstruction include:

*   **General Training:**
    *   `--total-steps <N>`: Total optimization iterations.
    *   `--ssim-weight <W>`: Balance between L1 and SSIM loss (default: 0.2).
*   **Learning Rates:**
    *   `--lr-mean <LR>`, `--lr-mean-end <LR>`: Learning rate for Gaussian positions (xyz).
    *   `--lr-coeffs-dc <LR>`, `--lr-coeffs-sh-scale <S>`: Learning rates for Spherical Harmonic coefficients (color).
    *   `--lr-opac <LR>`: Learning rate for opacity.
    *   `--lr-scale <LR>`, `--lr-scale-end <LR>`: Learning rate for Gaussian scale.
    *   `--lr-rotation <LR>`: Learning rate for Gaussian rotation.
*   **Regularization/Loss:**
    *   `--opac-loss-weight <W>`: Weight for opacity regularization loss.
    *   `--mean-noise-weight <W>`: Controls noise added to low-opacity Gaussians.
    *   `--match-alpha-weight <W>`: Weight for matching input image transparency.
*   **Adaptive Density Control (Refinement):**
    *   `--refine-every <N>`: Frequency (in steps) of densification/pruning.
    *   `--growth-grad-threshold <T>`: Gradient threshold for triggering densification.
    *   `--growth-select-fraction <F>`: Fraction of eligible Gaussians to actually densify.
    *   `--growth-stop-iter <N>`: Iteration after which densification stops.
*   **Model:**
    *   `--sh-degree <D>`: Degree of Spherical Harmonics used for view-dependent color (default: 3).
    *   `--max-splats <N>`: Hard upper limit on the number of Gaussians.
*   **Dataset:**
    *   `--max-frames <N>`, `--max-resolution <R>`, `--subsample-frames <N>`: Control dataset size/resolution.

*(Note: Default values are listed in the `--help` output. These parameters can significantly impact training time and final quality.)*

---

## Where to Go Next?

*   Understand how the scene is rendered: **[Gaussian Splat Rendering](gaussian-splatting.md)**.
*   See the high-level project structure: **[Architecture Overview](architecture.md)**.
*   Learn about the ML framework used: **[Burn in Core Technologies](core-technologies.md#343-burn)**.
*   Try running it: **[User Guide](../getting-started/user-guide.md)**. 